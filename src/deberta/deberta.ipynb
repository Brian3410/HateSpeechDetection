{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2023e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, fbeta_score\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Binary/multiclass metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    weighted_precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    weighted_recall = recall_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    # Class-specific metrics (assuming labels: 0 = No Hate, 1 = Hate)\n",
    "    f1_hate = f1_score(labels, preds, pos_label=1)\n",
    "    f1_no_hate = f1_score(labels, preds, pos_label=0)\n",
    "    precision_hate = precision_score(labels, preds, pos_label=1)\n",
    "    recall_hate = recall_score(labels, preds, pos_label=1)\n",
    "\n",
    "    # F0.5 and F2 scores\n",
    "    f05 = fbeta_score(labels, preds, average=\"binary\", beta=0.5)\n",
    "    f2 = fbeta_score(labels, preds, average=\"binary\", beta=2)\n",
    "\n",
    "    # AUC (only works if binary classification & probs available)\n",
    "    try:\n",
    "        probs = logits[:, 1]  # take score for class 1\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"f1_hate\": f1_hate,\n",
    "        \"f1_no_hate\": f1_no_hate,\n",
    "        \"f0.5\": f05,\n",
    "        \"f2\": f2,\n",
    "        \"auc\": auc,\n",
    "        \"precision_hate\": precision_hate,\n",
    "        \"recall_hate\": recall_hate,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "    }\n",
    "\n",
    "def train_deberta(aug, data_source):\n",
    "\n",
    "    dataset = load_dataset(\"csv\", keep_in_memory=True, data_files={\n",
    "        \"train\": f\"{aug}/{data_source}/train.csv\",\n",
    "        \"validation\": f\"{aug}/{data_source}/train.csv\",\n",
    "    })\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    checkpoint = \"microsoft/deberta-v3-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    # model.to(\"cuda\")\n",
    "    # Tokenize\n",
    "    def preprocess(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    encoded = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import (\n",
    "        f1_score, precision_score, recall_score,\n",
    "        roc_auc_score, accuracy_score, fbeta_score\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./deberta_{data_source}_{aug}_output\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=48,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        logging_first_step=True,\n",
    "        log_level=\"info\",\n",
    "        disable_tqdm=False, \n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        fp16=True,   # if you have a GPU with mixed precision\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded[\"train\"],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    except:\n",
    "        trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19302d72-8836-47a2-a3b4-9809a3c79e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deberta(\"tda\", \"corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_latest_checkpoint(folder_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Returns the path to the checkpoint with the largest number\n",
    "    in the format checkpoint-* inside the given folder.\n",
    "    If no checkpoints are found, returns None.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"^checkpoint-(\\d+)$\")\n",
    "    checkpoints = []\n",
    "\n",
    "    for name in os.listdir(folder_path):\n",
    "        match = pattern.match(name)\n",
    "        if match:\n",
    "            checkpoints.append((int(match.group(1)), name))\n",
    "\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    # Find max by checkpoint number\n",
    "    _, latest = max(checkpoints, key=lambda x: x[0])\n",
    "    return os.path.join(folder_path, latest)\n",
    "\n",
    "def perform_deberta_inference(aug, data_source):\n",
    "    # Load model from a specific checkpoint\n",
    "    \n",
    "    checkpoint_path = get_latest_checkpoint(f\"deberta_{data_source}_{aug}_output\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\":f\"{aug}/{data_source}/train.csv\",\"test\":f\"{aug}/{data_source}/test.csv\"})\n",
    "    # Tokenize\n",
    "    def preprocess(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    encoded = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./deberta_{data_source}_{aug}_output\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,   # if you have a GPU with mixed precision\n",
    "    )\n",
    "\n",
    "    # Recreate trainer with this model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=encoded[\"test\"],\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Runs evaluation loop with compute_metrics\n",
    "    test_metrics = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2453a34-8dda-49fc-b618-136827cd2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, fbeta_score\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Binary/multiclass metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    weighted_precision = precision_score(labels, preds, average=\"weighted\")\n",
    "    weighted_recall = recall_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    # Class-specific metrics (assuming labels: 0 = No Hate, 1 = Hate)\n",
    "    f1_hate = f1_score(labels, preds, pos_label=1)\n",
    "    f1_no_hate = f1_score(labels, preds, pos_label=0)\n",
    "    precision_hate = precision_score(labels, preds, pos_label=1)\n",
    "    recall_hate = recall_score(labels, preds, pos_label=1)\n",
    "\n",
    "    # F0.5 and F2 scores\n",
    "    f05 = fbeta_score(labels, preds, average=\"binary\", beta=0.5)\n",
    "    f2 = fbeta_score(labels, preds, average=\"binary\", beta=2)\n",
    "\n",
    "    # AUC (only works if binary classification & probs available)\n",
    "    try:\n",
    "        probs = logits[:, 1]  # take score for class 1\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"f1_hate\": f1_hate,\n",
    "        \"f1_no_hate\": f1_no_hate,\n",
    "        \"f0.5\": f05,\n",
    "        \"f2\": f2,\n",
    "        \"auc\": auc,\n",
    "        \"precision_hate\": precision_hate,\n",
    "        \"recall_hate\": recall_hate,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "    }\n",
    "\n",
    "def get_latest_checkpoint(folder_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Return path to the highest-numbered 'checkpoint-*' dir in folder_path, or None if none exist.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return None\n",
    "    pattern = re.compile(r\"^checkpoint-(\\d+)$\")\n",
    "    nums_and_names = []\n",
    "    for name in os.listdir(folder_path):\n",
    "        m = pattern.match(name)\n",
    "        if m:\n",
    "            nums_and_names.append((int(m.group(1)), name))\n",
    "    if not nums_and_names:\n",
    "        return None\n",
    "    _, latest = max(nums_and_names, key=lambda x: x[0])\n",
    "    return os.path.join(folder_path, latest)\n",
    "\n",
    "def perform_deberta_inference(aug: str, data_source: str, compute_metrics=None):\n",
    "    # --- Load model (prefer the latest checkpoint if present) ---\n",
    "    output_dir = f\"deberta_{data_source}_{aug}_output\"\n",
    "    ckpt_path = get_latest_checkpoint(output_dir) or output_dir  # fall back to base dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(ckpt_path)\n",
    "    if aug == \"smote\":\n",
    "        aug = \"original\"\n",
    "    elif aug == \"pos_smote\":\n",
    "        aug = \"pos_tagging\"\n",
    "    \n",
    "    # --- Load data ---\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": f\"original/{data_source}/train.csv\",\n",
    "            \"test\":  f\"original/{data_source}/test.csv\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # --- Tokenize (dynamic padding; keep labels) ---\n",
    "    def preprocess(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "    encoded = dataset.map(preprocess, batched=True)\n",
    "\n",
    "    # If your CSV has 'label' (singular), HF Trainer handles it.\n",
    "    # If it's named differently, rename here:\n",
    "    # encoded = encoded.rename_column(\"your_label_col\", \"label\")\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # --- Args (note: it's evaluation_strategy) ---\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        fp16=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=50,\n",
    "        # Below only matter if you train again; harmless for evaluate():\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # --- Trainer (wire the *tokenized* eval set, tokenizer, and collator) ---\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        eval_dataset=encoded[\"test\"],     # <- tokenized split\n",
    "        processing_class=tokenizer,              # <- so Trainer knows how to pad\n",
    "        data_collator=data_collator,      # <- dynamic padding\n",
    "        compute_metrics=compute_metrics,  # optional\n",
    "    )\n",
    "\n",
    "    # Use the default eval_dataset already set above\n",
    "    test_metrics = trainer.evaluate()\n",
    "    # Explicit cleanup\n",
    "    del trainer, model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44915699-cd45-4dd8-a355-dd995b897057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "result = perform_deberta_inference(\"tda\", \"corpus\", compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5537d26-c3ca-45cf-9253-72e315574c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a419c57-c458-42e3-953d-8b9c19d9e660",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perform_deberta_inference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m aug \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpos_tagging\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msmote\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpos_smote\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcorpus\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstormfront\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mreddit_gab\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munified\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         result = \u001b[43mperform_deberta_inference\u001b[49m(aug, dataset, compute_metrics)\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33munweighted_results.jsonl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m             f.write(json.dumps({\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maug\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: result}, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'perform_deberta_inference' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "for aug in [\"original\", \"pos_tagging\", \"smote\", \"pos_smote\"]:\n",
    "    for dataset in [\"corpus\", \"stormfront\", \"reddit_gab\", \"unified\"]:\n",
    "        result = perform_deberta_inference(aug, dataset, compute_metrics)\n",
    "        with open(\"unweighted_results.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({f\"{dataset}_{aug}\": result}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa2cfe-e634-4f4e-89ad-f35411fc86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Write dict to JSON file\n",
    "with open(\"unweighted_results.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bf3d8e-f17b-47e7-8a2f-11a2dad862e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ---- same compute_metrics you already defined ----\n",
    "# def compute_metrics(eval_pred): ...\n",
    "\n",
    "def evaluate_deberta(aug: str, data_source: str, ckpt_dir: str | None = None):\n",
    "    \"\"\"\n",
    "    Evaluate on {aug}/{data_source}/test.csv using your saved model.\n",
    "    - ckpt_dir: path to the folder you saved (defaults to train output dir).\n",
    "    - Writes metrics to stdout and a CSV with per-row predictions.\n",
    "    \"\"\"\n",
    "    test_csv = f\"original/{data_source}/test.csv\"\n",
    "    out_dir  = ckpt_dir or f\"deberta_{data_source}_{aug}_output\"\n",
    "    assert os.path.exists(out_dir), f\"Checkpoint folder not found: {out_dir}\"\n",
    "    assert os.path.exists(test_csv), f\"Test CSV not found: {test_csv}\"\n",
    "\n",
    "    # ---- Load tokenizer & model from your saved folder ----\n",
    "    tokenizer = AutoTokenizer.from_pretrained(out_dir, use_fast=True)\n",
    "    tokenizer.model_max_length = 256\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        out_dir,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n",
    "    )\n",
    "\n",
    "    # ---- Build test dataset (keep original text to export later) ----\n",
    "    df_test = pd.read_csv(test_csv, dtype={\"text\": \"string\", \"label\": \"int64\"})\n",
    "    df_test = df_test.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    test_ds_raw = Dataset.from_pandas(df_test, preserve_index=False)\n",
    "    test_enc    = test_ds_raw.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    # ---- Minimal eval args (no extra logging/saving) ----\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"eval_tmp\"),\n",
    "        per_device_eval_batch_size=256,\n",
    "        dataloader_num_workers=0,\n",
    "        fp16=False, bf16=True,#torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        logging_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,   # <- reuses your metrics\n",
    "    )\n",
    "\n",
    "    # Option A: full prediction object (preds + metrics)\n",
    "    pred_out = trainer.predict(test_enc)\n",
    "    logits = pred_out.predictions\n",
    "    y_true = np.array(df_test[\"label\"].tolist())\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(\"=== Metrics from compute_metrics ===\")\n",
    "    for k, v in pred_out.metrics.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # (Optional) nice sklearn report\n",
    "    print(\"\\n=== Classification report ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    # ---- Save per-row predictions (with text) ----\n",
    "    # If you want probability of the positive class for inspection/AUC plots:\n",
    "    # use softmax for well-calibrated probabilities\n",
    "    probs_pos = (np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True))[:, 1]\n",
    "\n",
    "    out_csv = os.path.join(out_dir, \"test_predictions.csv\")\n",
    "    pd.DataFrame({\n",
    "        \"text\": df_test[\"text\"],\n",
    "        \"label\": y_true,\n",
    "        \"pred\": y_pred,\n",
    "        \"prob_pos\": probs_pos,\n",
    "    }).to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nðŸ“„ Saved per-sample predictions to: {out_csv}\")\n",
    "\n",
    "    return pred_out.metrics  # dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a70ee5-bd71-4f5e-ade9-a4675590b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e801007bce47dcbb3258d424fb1b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/ml23/stefans1/conda/envs/python312/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metrics from compute_metrics ===\n",
      "test_loss: 0.9114176034927368\n",
      "test_accuracy: 0.6905527932143902\n",
      "test_macro_f1: 0.4537909811311638\n",
      "test_f1_hate: 0.09417808219178082\n",
      "test_f1_no_hate: 0.8134038800705468\n",
      "test_f0.5: 0.12415349887133183\n",
      "test_f2: 0.07586206896551724\n",
      "test_auc: 0.5017793744716822\n",
      "test_precision_hate: 0.15759312320916904\n",
      "test_recall_hate: 0.06715506715506715\n",
      "test_weighted_f1: 0.6411178524417929\n",
      "test_weighted_precision: 0.6089596189790631\n",
      "test_weighted_recall: 0.6905527932143902\n",
      "test_runtime: 5.4648\n",
      "test_samples_per_second: 625.642\n",
      "test_steps_per_second: 0.915\n",
      "\n",
      "=== Classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7511    0.8869    0.8134      2600\n",
      "           1     0.1576    0.0672    0.0942       819\n",
      "\n",
      "    accuracy                         0.6906      3419\n",
      "   macro avg     0.4544    0.4770    0.4538      3419\n",
      "weighted avg     0.6090    0.6906    0.6411      3419\n",
      "\n",
      "\n",
      "ðŸ“„ Saved per-sample predictions to: deberta_corpus_tda_output/test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e60a38a3f2048a582dd14242e8547f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/ml23/stefans1/conda/envs/python312/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metrics from compute_metrics ===\n",
      "test_loss: 0.4506986439228058\n",
      "test_accuracy: 0.9287671232876712\n",
      "test_macro_f1: 0.7996876055386694\n",
      "test_f1_hate: 0.6388888888888888\n",
      "test_f1_no_hate: 0.9604863221884499\n",
      "test_f0.5: 0.6845238095238095\n",
      "test_f2: 0.5989583333333334\n",
      "test_auc: 0.9198461538461538\n",
      "test_precision_hate: 0.71875\n",
      "test_recall_hate: 0.575\n",
      "test_weighted_f1: 0.9252427678542514\n",
      "test_weighted_precision: 0.9237216668723518\n",
      "test_weighted_recall: 0.9287671232876712\n",
      "test_runtime: 0.8495\n",
      "test_samples_per_second: 1289.025\n",
      "test_steps_per_second: 2.354\n",
      "\n",
      "=== Classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9489    0.9723    0.9605       975\n",
      "           1     0.7188    0.5750    0.6389       120\n",
      "\n",
      "    accuracy                         0.9288      1095\n",
      "   macro avg     0.8338    0.7737    0.7997      1095\n",
      "weighted avg     0.9237    0.9288    0.9252      1095\n",
      "\n",
      "\n",
      "ðŸ“„ Saved per-sample predictions to: deberta_stormfront_tda_output/test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a331b8dd4c4399b12678b038339885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/ml23/stefans1/conda/envs/python312/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metrics from compute_metrics ===\n",
      "test_loss: 0.4267905056476593\n",
      "test_accuracy: 0.9029390420899854\n",
      "test_macro_f1: 0.8962149819444716\n",
      "test_f1_hate: 0.8697980043806279\n",
      "test_f1_no_hate: 0.9226319595083152\n",
      "test_f0.5: 0.8529832935560859\n",
      "test_f2: 0.8872889771598809\n",
      "test_auc: 0.9530866233353679\n",
      "test_precision_hate: 0.8421300659754948\n",
      "test_recall_hate: 0.8993457473578259\n",
      "test_weighted_f1: 0.9035860471645717\n",
      "test_weighted_precision: 0.9053606384839253\n",
      "test_weighted_recall: 0.9029390420899854\n",
      "test_runtime: 4.3757\n",
      "test_samples_per_second: 1259.681\n",
      "test_steps_per_second: 1.828\n",
      "\n",
      "=== Classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9410    0.9050    0.9226      3525\n",
      "           1     0.8421    0.8993    0.8698      1987\n",
      "\n",
      "    accuracy                         0.9029      5512\n",
      "   macro avg     0.8916    0.9022    0.8962      5512\n",
      "weighted avg     0.9054    0.9029    0.9036      5512\n",
      "\n",
      "\n",
      "ðŸ“„ Saved per-sample predictions to: deberta_reddit_gab_tda_output/test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97285fe176964b15b47bbc345c4de5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8276 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch2/ml23/stefans1/conda/envs/python312/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metrics from compute_metrics ===\n",
      "test_loss: 0.6262568235397339\n",
      "test_accuracy: 0.862010633156114\n",
      "test_macro_f1: 0.8455469833230738\n",
      "test_f1_hate: 0.7951202009329028\n",
      "test_f1_no_hate: 0.8959737657132447\n",
      "test_f0.5: 0.7985585585585585\n",
      "test_f2: 0.7917113254733833\n",
      "test_auc: 0.9215170938208381\n",
      "test_precision_hate: 0.8008673653776653\n",
      "test_recall_hate: 0.7894549340933381\n",
      "test_weighted_f1: 0.8617669077699848\n",
      "test_weighted_precision: 0.8615666813116527\n",
      "test_weighted_recall: 0.862010633156114\n",
      "test_runtime: 6.3384\n",
      "test_samples_per_second: 1305.689\n",
      "test_steps_per_second: 1.735\n",
      "\n",
      "=== Classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8927    0.8993    0.8960      5469\n",
      "           1     0.8009    0.7895    0.7951      2807\n",
      "\n",
      "    accuracy                         0.8620      8276\n",
      "   macro avg     0.8468    0.8444    0.8455      8276\n",
      "weighted avg     0.8616    0.8620    0.8618      8276\n",
      "\n",
      "\n",
      "ðŸ“„ Saved per-sample predictions to: deberta_unified_tda_output/test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"corpus\", \"stormfront\", \"reddit_gab\", \"unified\"]:\n",
    "    evaluate_deberta(\"tda\", dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
